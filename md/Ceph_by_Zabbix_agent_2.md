# Ceph by Zabbix agent 2 template description

You can discuss this template or leave feedback on our forum https://www.zabbix.com/forum/zabbix-suggestions-and-feedback/410059-discussion-thread-for-official-zabbix-template-ceph

Generated by official Zabbix template tool "Templator"

## Summary
* [items](#items)
* [macros](#macros)
* [triggers](#triggers)
* [discoveries](#discoveries)
  * [Discovery OSD ](#discovery_osd)
  * [Discovery Pool ](#discovery_pool)

<a name="items"></a>

## Items
| name | description | key | type | delay |
| ------------- |------------- |------------- |------------- |------------- |
| Get df | no description | ceph.df.details["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no type | no delay |
| Minimum Mon release version | min_mon_release_name | ceph.min_mon_release_name | DEPENDENT | 0 |
| Number of Monitors | The number of Monitors configured in a Ceph cluster. | ceph.num_mon | DEPENDENT | 0 |
| Number of OSDs | The number of the known storage daemons in a Ceph cluster. | ceph.num_osd | DEPENDENT | 0 |
| Number of OSDs in state: IN | The total number of the participating storage daemons in a Ceph cluster. | ceph.num_osd_in | DEPENDENT | 0 |
| Number of OSDs in state: UP | The total number of the online storage daemons in a Ceph cluster. | ceph.num_osd_up | DEPENDENT | 0 |
| Number of Placement Groups | The total number of Placement Groups in a Ceph cluster. | ceph.num_pg | DEPENDENT | 0 |
| Number of Placement Groups in Temporary state | The total number of Placement Groups in a *pg_temp* state | ceph.num_pg_temp | DEPENDENT | 0 |
| Number of Pools | The total number of pools in a Ceph cluster. | ceph.num_pools | DEPENDENT | 0 |
| Get OSD dump | no description | ceph.osd.dump["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no type | no delay |
| Get OSD stats | no description | ceph.osd.stats["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no type | no delay |
| Ceph backfill full ratio | The backfill full ratio setting of the Ceph cluster as configured on OSDMap. | ceph.osd_backfillfull_ratio | DEPENDENT | 0 |
| Ceph OSD avg fill | The average fill of OSDs. | ceph.osd_fill.avg | DEPENDENT | 0 |
| Ceph OSD max fill | The percentage of the most filled OSD. | ceph.osd_fill.max | DEPENDENT | 0 |
| Ceph OSD min fill | The percentage fill of the minimum filled OSD. | ceph.osd_fill.min | DEPENDENT | 0 |
| Ceph full ratio | The full ratio setting of the Ceph cluster as configured on OSDMap. | ceph.osd_full_ratio | DEPENDENT | 0 |
| Ceph OSD Apply latency Avg | The average apply latency of OSDs. | ceph.osd_latency_apply.avg | DEPENDENT | 0 |
| Ceph OSD Apply latency Max | The maximum apply latency of OSDs. | ceph.osd_latency_apply.max | DEPENDENT | 0 |
| Ceph OSD Apply latency Min | The minimum apply latency of OSDs. | ceph.osd_latency_apply.min | DEPENDENT | 0 |
| Ceph OSD Commit latency Avg | The average commit latency of OSDs. | ceph.osd_latency_commit.avg | DEPENDENT | 0 |
| Ceph OSD Commit latency Max | The maximum commit latency of OSDs. | ceph.osd_latency_commit.max | DEPENDENT | 0 |
| Ceph OSD Commit latency Min | The minimum commit latency of OSDs. | ceph.osd_latency_commit.min | DEPENDENT | 0 |
| Ceph nearfull ratio | The near full ratio setting of the Ceph cluster as configured on OSDMap. | ceph.osd_nearfull_ratio | DEPENDENT | 0 |
| Ceph OSD avg PGs | The average amount of Placement Groups on OSDs. | ceph.osd_pgs.avg | DEPENDENT | 0 |
| Ceph OSD max PGs | The maximum amount of Placement Groups on OSDs. | ceph.osd_pgs.max | DEPENDENT | 0 |
| Ceph OSD min PGs | The minimum amount of Placement Groups on OSDs. | ceph.osd_pgs.min | DEPENDENT | 0 |
| Overall cluster status | The overall Ceph cluster status, eg 0 - HEALTH_OK, 1 - HEALTH_WARN or 2 - HEALTH_ERR. | ceph.overall_status | DEPENDENT | 0 |
| Number of Placement Groups in Active state | The total number of Placement Groups in an active state. | ceph.pg_states.active | DEPENDENT | 0 |
| Number of Placement Groups in Backfilling state | The total number of Placement Groups in a backfill state. | ceph.pg_states.backfilling | DEPENDENT | 0 |
| Number of Placement Groups in backfill_toofull state | The total number of Placement Groups in a *backfill_toofull state*. | ceph.pg_states.backfill_toofull | DEPENDENT | 0 |
| Number of Placement Groups in backfill_wait state | The total number of Placement Groups in a *backfill_wait* state. | ceph.pg_states.backfill_wait | DEPENDENT | 0 |
| Number of Placement Groups in Clean state | The total number of Placement Groups in a clean state. | ceph.pg_states.clean | DEPENDENT | 0 |
| Number of Placement Groups in degraded state | The total number of Placement Groups in a degraded state. | ceph.pg_states.degraded | DEPENDENT | 0 |
| Number of Placement Groups in inconsistent state | The total number of Placement Groups in an inconsistent state. | ceph.pg_states.inconsistent | DEPENDENT | 0 |
| Number of Placement Groups in Peering state | The total number of Placement Groups in a peering state. | ceph.pg_states.peering | DEPENDENT | 0 |
| Number of Placement Groups in recovering state | The total number of Placement Groups in a recovering state. | ceph.pg_states.recovering | DEPENDENT | 0 |
| Number of Placement Groups in recovery_wait state | The total number of Placement Groups in a *recovery_wait* state. | ceph.pg_states.recovery_wait | DEPENDENT | 0 |
| Number of Placement Groups in remapped state | The total number of Placement Groups in a remapped state. | ceph.pg_states.remapped | DEPENDENT | 0 |
| Number of Placement Groups in Scrubbing state | The total number of Placement Groups in a scrubbing state. | ceph.pg_states.scrubbing | DEPENDENT | 0 |
| Number of Placement Groups in Undersized state | The total number of Placement Groups in an undersized state. | ceph.pg_states.undersized | DEPENDENT | 0 |
| Number of Placement Groups in Unknown state | The total number of Placement Groups in an unknown state. | ceph.pg_states.unknown | DEPENDENT | 0 |
| Ping | no description | ceph.ping["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no type | no delay |
| Ceph Read bandwidth | The global read bytes per second. | ceph.rd_bytes.rate | DEPENDENT | 0 |
| Ceph Read operations per sec | The global read operations per second. | ceph.rd_ops.rate | DEPENDENT | 0 |
| Get overall cluster status | no description | ceph.status["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no type | no delay |
| Total bytes available | The total bytes available in a Ceph cluster. | ceph.total_avail_bytes | DEPENDENT | 0 |
| Total bytes | The total (RAW) capacity of a Ceph cluster in bytes. | ceph.total_bytes | DEPENDENT | 0 |
| Total number of objects | The total number of objects in a Ceph cluster. | ceph.total_objects | DEPENDENT | 0 |
| Total bytes used | The total bytes used in a Ceph cluster. | ceph.total_used_bytes | DEPENDENT | 0 |
| Ceph Write bandwidth | The global write bytes per second. | ceph.wr_bytes.rate | DEPENDENT | 0 |
| Ceph Write operations per sec | The global write operations per second. | ceph.wr_ops.rate | DEPENDENT | 0 |


<a name="macros"></a>

## Macros
| macro | value |
| ------------- |------------- |
| {$CEPH.API.KEY} | zabbix_pass |
| {$CEPH.CONNSTRING} | https://localhost:8003 |
| {$CEPH.USER} | zabbix |


<a name="triggers"></a>

## Triggers
| name | priority | description | expression | tags | url |
| ------------- |------------- |------------- |------------- |------------- |------------- |
| Minimum monitor release version has changed | INFO | A Ceph version has changed. Acknowledge to close the problem manually. | last(/Ceph by Zabbix agent 2/ceph.min_mon_release_name,#1)<>last(/Ceph by Zabbix agent 2/ceph.min_mon_release_name,#2) and length(last(/Ceph by Zabbix agent 2/ceph.min_mon_release_name))>0 | [{"tag": "scope", "value": "notice"}] | no url |
| Cluster in ERROR state | AVERAGE | no description | last(/Ceph by Zabbix agent 2/ceph.overall_status)=2 | [{"tag": "scope", "value": "availability"}] | no url |
| Cluster in WARNING state | WARNING | no description | last(/Ceph by Zabbix agent 2/ceph.overall_status)=1 | [{"tag": "scope", "value": "availability"}] | no url |
| Can not connect to cluster | AVERAGE | The connection to the Ceph RESTful module is broken (if there is any error presented including *AUTH* and the configuration issues). | last(/Ceph by Zabbix agent 2/ceph.ping["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"])=0 | [{"tag": "scope", "value": "availability"}] | no url |


<a name="discoveries"></a>

## Discoveries
| name | key | description | type | lifetime | delay |
| ------------- |------------- |------------- |------------- |------------- |------------- |
| OSD | ceph.osd.discovery["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no description | no type | no lifetime | 1h |
| Pool | ceph.pool.discovery["{$CEPH.CONNSTRING}","{$CEPH.USER}","{$CEPH.API.KEY}"] | no description | no type | no lifetime | 1h |


<a name="discovery_osd" />

## Discovery OSD

### Items

| name | description | key | type |
| ------------- |------------- |------------- |------------- |
| [osd.{#OSDNAME}] OSD fill | no description | ceph.osd[{#OSDNAME},fill] | DEPENDENT |
| [osd.{#OSDNAME}] OSD in | no description | ceph.osd[{#OSDNAME},in] | DEPENDENT |
| [osd.{#OSDNAME}] OSD latency apply | The time taken to flush an update to disks. | ceph.osd[{#OSDNAME},latency_apply] | DEPENDENT |
| [osd.{#OSDNAME}] OSD latency commit | The time taken to commit an operation to the journal. | ceph.osd[{#OSDNAME},latency_commit] | DEPENDENT |
| [osd.{#OSDNAME}] OSD PGs | no description | ceph.osd[{#OSDNAME},num_pgs] | DEPENDENT |
| [osd.{#OSDNAME}] OSD up | no description | ceph.osd[{#OSDNAME},up] | DEPENDENT |


### Triggers

| name | priority | description | expression | tags | url |
| ------------- |------------- |------------- |------------- |------------- |------------- |
| OSD osd.{#OSDNAME} is down | AVERAGE | OSD osd.{#OSDNAME} is marked "down" in the *osdmap*.<br>The OSD daemon may have been stopped, or peer OSDs may be unable to reach the OSD over the network. | last(/Ceph by Zabbix agent 2/ceph.osd[{#OSDNAME},up]) = 0 | [{"tag": "scope", "value": "availability"}] | no url |
| Ceph OSD osd.{#OSDNAME} is near full | WARNING | no description | min(/Ceph by Zabbix agent 2/ceph.osd[{#OSDNAME},fill],15m) > last(/Ceph by Zabbix agent 2/ceph.osd_nearfull_ratio)*100 | [{"tag": "scope", "value": "capacity"}] | no url |
| OSD osd.{#OSDNAME} is full | AVERAGE | no description | min(/Ceph by Zabbix agent 2/ceph.osd[{#OSDNAME},fill],15m) > last(/Ceph by Zabbix agent 2/ceph.osd_full_ratio)*100 | [{"tag": "scope", "value": "capacity"}] | no url |


<a name="discovery_pool" />

## Discovery Pool

### Items

| name | description | key | type |
| ------------- |------------- |------------- |------------- |
| [{#POOLNAME}] Pool Used | The total bytes used in a pool. | ceph.pool["{#POOLNAME}",bytes_used] | DEPENDENT |
| [{#POOLNAME}] Max available | The maximum available space in the given pool. | ceph.pool["{#POOLNAME}",max_avail] | DEPENDENT |
| [{#POOLNAME}] Pool objects | The number of objects in the pool. | ceph.pool["{#POOLNAME}",objects] | DEPENDENT |
| [{#POOLNAME}] Pool Percent Used | The percentage of the storage used per pool. | ceph.pool["{#POOLNAME}",percent_used] | DEPENDENT |
| [{#POOLNAME}] Pool Read bandwidth | The read rate per pool (bytes per second). | ceph.pool["{#POOLNAME}",rd_bytes.rate] | DEPENDENT |
| [{#POOLNAME}] Pool Read operations | The read rate per pool (operations per second). | ceph.pool["{#POOLNAME}",rd_ops.rate] | DEPENDENT |
| [{#POOLNAME}] Pool RAW Used | Bytes used in pool including the copies made. | ceph.pool["{#POOLNAME}",stored_raw] | DEPENDENT |
| [{#POOLNAME}] Pool Write bandwidth | The write rate per pool (bytes per second). | ceph.pool["{#POOLNAME}",wr_bytes.rate] | DEPENDENT |
| [{#POOLNAME}] Pool Write operations | The write rate per pool (operations per second). | ceph.pool["{#POOLNAME}",wr_ops.rate] | DEPENDENT |

